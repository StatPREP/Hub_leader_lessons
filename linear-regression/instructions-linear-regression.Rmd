---
title: "The two-sample t test"
author: "Carol Howald"
date: "version 0.2 `r Sys.Date()`"
output:
  rmarkdown::word_document: 
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
    includes:
      in_header: header.tex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

## Orientation for instructors

Linear regression is one of the oldest and most widely used statistical techniques. It is used to describe or *model* a connection or relationship between a quantitative *response variable* and one or more *explanatory variables*. The notes in  this section 

Many, perhaps most, introductory statistics courses cover *simple regression*, which is a special case of linear regression in which the response variable, $y$ is modeled as a straight-line function of the explanatory variable $x$, that  is, $y = f(x) = m x + b$. The slope $m$ and intercept $b$, constitute a concise but very limited way of describing important features of the relationship between the response and explanatory variables.

## Role in statistical practice

Statistics instructors, especially those whose training is primarily in mathematics, don't always have experience with actual applications of statistics. Understandably, such instructors use their theoretical understanding and familiarity with textbook problems to try to imagine how statistical methods are applied. The result can be misconceptions about the role of the methods being taught in contemporary work. In order to teach statistics in a data-centric way, it's important to recognize such misconceptions. A better inf 


## Conceptual pitfalls

There are many potential pitfalls in teaching about simple regression. One has to do with nomenclature.  Mathematicians describe $m$ and $b$ as "coefficients" or  "parameters." In statistics, the meaning of "parameter" is different (referring to a population) and the values of $m$ and $b$ generated by regression are "statistics" (referring to a sample from the population). And a "coefficient" in a formula like $m x + b$ is not particularly similar to a "correlation coefficient." Another pitfall has to do with the distinction between descriptive and inferential statistics. That's a valuable distinction to make, but a focus on the correlation coefficient tends to mix the distinct concepts together. See the section on "pushing the envelope" for a discussion on common misconceptions held by instructors.

One such misconception that it's worth discussing here is the role of simple regression in genuine statistical work. It's fair to say that simple regression is too simple to support contemporary research and has been for some decades. It is uncommon for there to be just a single explanatory variable. A more general technique, *multiple regression*, supports the use of multiple explanatory variables. Mathematics educators sometimes regard simple regression as a useful special case to introduce regression to students. If one looks at things from linear algebra involved in the calculations (e.g. pseudo-inverses), this is true. But simple regression doesn't provide any traction on one of the most important reasons multiple regression is used: "adjusting" or "controlling for" covariates.

## Objectives for studying linear regression

* Create scatterplots for bivariate data using graphing technology where appropriate
* Identify predictor and response (independent/dependent) variables.  [This is  a signal to our faculty about what to assess people on.] 

    - External evidence of which  direction  causation goes, e.g. hours work explains total pay.
    - Response quantitative (trivial)
    - Legend is predictor should be quantitative or at least ordinal. Reality: it can be anything.
    - Why are you making a prediction:
        - Deduce from something easy to measure, something that would be hard to measure. Future.
        - Hypothesis formation.
        
    
* Determine whether a straight-line model is appropriate for describing a given relationship.
    - Students can distinguish between situations where the relationship is approximately linear and when it is not. Examples: Height versus  age, BMI vs weight, BMI versus height (which has a crazy, upsidedown whistle-shaped cloud)
    - residual, e.g.  heteroscedasticity
    - covariate

* Interpret the correlation coefficient in terms of pos/neg/null and strength of correlation
* Interpret the slope of the regression *equation** or the **function**, **model**, **formula**
    - incremental change. 
    
* Translate a difference in the input to the corresponding difference in the output. (Rule of 4 from calculus reform.)
    - from the graph
    - from the regression coefficient
    - Effect size, 
    - What's a big change in put (a  couple of SD of x), what's a strong relationship: results in a big change in the output (a couple of SD of y). Correlation coefficient is directly in  terms of translation of SD  in input to  SD in  output.

* Use the regression equation for prediction 
    - plug in inputs to get an output
    - avoid extrapolation as unsafe
    - must include  the variation around the model.
    

* Use technology to find linear regression models and correlation coefficients for a data set
* Understand the pitfalls of extrapolation

* Be able to make a point plot using technology and to relate the location of each point to the corresponding row in a data table.
* Develop an intuition for how a mathematical function can describe the pattern in a point-plot cloud.
* Recognize settings and variables for which regression is an appropriate technique.
* Sensibly choose which variable should be the response and which the explanatory variable, and know when it does and doesn't matter.
* Use technology to carry out linear regression.
* Be able to use the slope as a concise description of a relationship.
* Recognize what residuals from a regression model have to say.
* Understand how a regression model can be used for prediction.
* Illustrate the generality of confidence intervals to statistical descriptions of all sorts and interpret the  confidence interval in terms of negative/null/positive  relationships.
* Insofar as the correlation coefficient is topic in your course  (and it  need not be!) ... establish the connections between  regression slopes and correlation coefficients.

## Student background

Students will need some background knowledge in order to follow lessons  on simple regression.

- Variable types: quantitative and categorical
- Point plot: (The term "scatter plot" has traditionally been used.)
    * each axis corresponds to a variable
    * each row is one dot. 
- Mathematical functions:
    * translate a given input to an output by plugging the input into an arithmetic formula
    * in writing the formula, we often use symbols, like $m$ and $b$ to represent quantities.
    * the straight-line function
        - slope (primary importance here)
        - intercept
- Understand distinctions between various reasons for examining relationship:
    * to make a prediction of the unknown value of a variable given the known values of other variables
    * to anticipate the result of an intervention (This is a form of prediction that assumes a specific causal relationship)
    * to demonstrate that two variables are connected in some way.
    * to explore data in order to frame hypotheses about how the system works. 
- Standard  deviations if using $r$. This is not central if focusing on slope and intercept.
- ~~Greek~~. If you're going to use sophisticated mathematical notation to convey concepts, you are assuming your students know something about that notation. This might include:
    * Greek letters and their Roman equivalents, e.g.  distinguishing among $\beta$ and $B$ and $b$ or between $\mu$ and  $m$ and remember that $\mu$ is not cognate to $u$.
    * The different meanings of subscripts and superscripts, e.g. the distinct meanings of $\beta^2$ (exponentiation) and $\beta_2$ (identifying one in a series).
    * The various (inconsistent and sometimes contradictory) notations for distinguishing  between estimates and population parameters: 
        - Parameters: $\beta$, $\mu$, $\sigma$, and informally $b$, $m$, $s$
        - Estimates: $\hat{\beta}$, $b$, $\hat{b}$, $\hat{\mu}$, $m$, $\bar{m}$, $s$, $\hat{\sigma}$ 

## Background data knowledge

## (If we had) the population

BMI vs age?

## Student tasks and activities

- Point plot and functions. In which we'll ask students to sketch  out some functions from prior  knowledge (e.g. height versus age ) and then indicate the range of values around the function. Then turn this  around so that you deduce  the function and range of residuals from the point plot.

- Explanatory vs response variable: prediction versus intervention vs description vs hypothesis formation. 

- From data to function.

- Slopes and differences.  
    - Don't use $y  = m x + b$ except as a reminder of what a slope is.  Instead ...
        - read the slope off a graph. Don't worry about the intercept.
        - read the slope off a regression report.
        - "the effect size of x on y"????
    - Differences: if the input changes, how much does the output change?
    
- With the app: Can we predict something hard to measure from something easy.
    - systolic blood pressure from height?
    - income from BMI

- With  the app: f(x) is not destiny. Predict BMI from education. The averages differ, but there is a big range around the line. Can't predict for an individual, could say something about averages in  a group.

- With the app: How much  variation  is explained?

- With the app: Nonlinear functions -- when we need them and when we don't. Do they add something to the explanation.

- With  the app: sample size and significance. Look at how $R^2$ changes as we add in  explanatory variables and nonlinearity.
    - it never goes down, usually  up.
    - it might change by a trivial amount: what can you say  about the difference between a 10th percentile case and a 90th percentile case? 25th  and 75th (which is the middle of the bottom half and of the top half).

## Creating an active classroom

Maybe a single link to active classroom tips, and one or  two specific examples for this lesson.

Demoing versus students in  a lab

## Assessment items

## Pushing the envelope/advancing the field

Understand the different settings in which regression is used in practice. A good topic for discussion in the workshop. Use examples from the different settings. 
  - causation
  - classification
  - exploration: what might explain body mass index? 

Defining  big in terms of a the individual variables, e.g. a couple of standard deviations. This relates to  the discussion of "interpreting slope."

A commonly used tricotomy for describing relationships between two variables is "negative" vs "zero"/"none" vs "positive". In the context of simple regression, these correspond to the sign of the slope $m$. This can  be misleading, since a zero value of $m$ can occur even  when there is a strong (nonlinear) relationship between $y$ and $x$.

The slope $m$ is a physical quantity that has dimension and units. For instance if $y$ is a person's height in cm, and $x$ is a person's weight in kg, the units of $m$ will be cm/kg. (The "dimension" of this is L/M -- length over mass.)  Many mathematical educators prefer to de-emphasize physical units, preferring to regard $m$ as a pure number. This is a mistake from a statistical point of view. The size of physical quantities is important. Interpreting $m$ as large or small needs to be understood in the context of the problem.

The correlation coefficient $r$ is a scaled version of $m$. The scaling is by the ratio of the standard deviations of the $x$ and $y$ variables, that is, $r = \frac{\sigma_x}{\sigma_y} m$. This scaling results in $r$ being a pure number since the units  of $\sigma_x  / \sigma_y$ cancel out the units of $m$.

The slope $m$ can be any numerical quantity. In contrast, the correlation coefficient must always be $-1 \leq r \leq 1$. Many mathematics educators believe that this means that $r$ describes the "strength" of the relationship between $y$ and $x$. Whether or  not this is  true depends  on  what one means by "strength." In scientific research, the intuition behind strength corresponds better to the slope $m$ and includes the physical units of $m$. In statistics, when "strength" is taken to refer to how compelling the evidence is for  a claim, an appropriate measure is the *confidence interval* on $m$. Another statistical quantity, the *p-value* on the slope, refers to a quantifying the evidence for a particular but very weak  sort of claim, that $m$ is anything but zero. 

Although students are often drilled in the fact that $-1 \leq r \leq 1$, the reason why $r$ is bounded in this way is subtle. It's misleading to conclude that the bounds on $r$ suggest that a "strong" relationship  is one there $|r| \approx 1$.  The correlation coefficient $r$ predates the distinction  between descriptive and inferential statistics and mixes together aspects of both. This leads to pedagogical challenges that could be avoided if relationships are described using $m$ and inferences made using the confidence interval on $m$.


* Too much is made of the "optimality" of the estimates of the slope and intercept.
* Categorical explanatory variables can also be used. ANOVA is a  general procedure in linear regression. Almost  every statistical method covered in intro stats -- proportions, differences in  proportions, means, differences in means, ANOVA -- can be presented quite naturally as a linear regression  problem.
* Robust statistical methods are available to deal automatically with outliers, without having to handle them as special cases.
* $r$ is meaningless in  multiple regression. $R^2$ is more general.
* Although $y$ and $x$ are conventional names given to the variables involved when discussing  statistical and mathematical theory, 

## Author info

